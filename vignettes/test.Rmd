---
title: "XcalRep Vignette"
author: "Nicholas Mikolajewicz"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}

# devtools::build_vignettes()
knitr::opts_chunk$set(
  collapse = F,
  comment = "#>"
)
```

# Introduction 

XcalRep (**x**-**cal**ibration & **rep**roducibility) is an R implementation of methods used for cross-calibration and precision analysis of HR-pQCT

It enables cross-calibration and precision analysis in R for  single- and multi-centre studies, conducted at single or multiple timepoints. XcalRep was developed with intended use for HR-pQCT, however, it easily extends to other instruments 
which undergo routine calibration and reproducibility assessment. 

## Terminology 

Throughout this vignette and accompanying paper (**citation placeholder**), we use the following terminology:

- **Features**: Data column variable, such as "value", "site", "timePoint" or "phantom".

- **Feature type**: A specific realization of a feature, sometimes referred to as type. E.g., "Toronto" and "Montreal" are two different feature types belonging to the "site" feature. 

- **parameter**: A specific type of HR-pQCT measurement, such as "Tb. vBMD" or "Tb. N". 

- **value**: Parameter measurement obtained by HR-pQCT. 

# Example Workflow

### Install XcalRep Package


```{r install XcalRep, message = F, warning = F}

# placeholder

```

### Load in XcalRep 

```{r load XcalRep, message = F, warning = F}
rm(list = ls()) # clear enviroment
library(XcalRep)
```

## 1. Create Calibration Object

### 1.1 Calibration Object
Calibration Objects are workhorse of the XcalRep package. Once input data is correctly speciifed and the Calibration Object initiated, all downstream transformations, analyses, results and plots will be stored and retrieved from the Calibration Object.

### 1.2 Import Data

Data should be prepared and provied as a data frame containing expected features (see **Section 1.3**). 
As a running example to demonstrate the functionality of XcalRep, We will use HR-pQCT phantom calibration data published in **citation placeholder**. This dataset is provided as a part of the XCalRep package. 


```{r input data}
# load data into global enviroment 
input.data <- efp

# show table header
showTable(input.data, head.flag = T) 
```

### 1.3 Expected input data
When initiating a Calibration Object, the following list of features below are expected:

- **value**: quantitative measure obtained by instrument. 

- **site**: Instrument used to obtain measurement values. E.g., 'Toronto' refers to single HR-pQCT scanner located in Toronto. 

- **scanID**: Non-unique identifier used to specify scanning set. E.g., triplciate phantom scans will have the same scanID. 

- **section**: Region in phantom corresponding to unique characteristics. Recommended for precision analyses, and required for cross-calibrations. 

- **timePoint**: Time point at which measures were obtained (e.g., baseline, 6 months, 12 months). Numerical and character values accepted

- **phantom**: Type of imaging phantom used (e.g., EFP, QC1). Can also be specified as *in vivo* scanned region (e.g., radius, tibia), thus allowing for short-term precision analysis of *in vivo* scans (e.g., replicate patient scans with full repositioning). 

- **parameter**: Specific type of instrument measurement. E.g., 'Tt. vBMD'. 

- **scanDate**: Date at which instrument measurements were obtained. 

Note that expected input are spelling- and case-sensitive. Make sure everything is correctly specified!

### 1.4 Implementation

```{r complete co}
# create Calibration Object
co <- createCalibrationObject(input.data)
```

In practice, not all datasets will have data for each expected feature. For example, a study may only be interested in quantifying the short-term precision of a single instrument. In such a case, the timePoint and site features can be omitted and the `Calibration Object` will issue a warning that the omitted feature was detected and a placeholder feature was created (this will not affect downstream analysis).

We can see how this is handled by XcalRep using an incomplete input dataset

```{r incomplete co}

# omit site, timePoint, and scanDate features
input.data.incomplete <- input.data %>% dplyr::select(-c("site", "timePoint", "scanDate"))

# create calibration object using incomplete data input
co.incomplete <- createCalibrationObject(input.data.incomplete)
```


## 2. Examining the Calibration Object

### 2.1 Assays

A central feature of `Calibration Objects` are `Assays`. When a `Calibration Object` is initiated, an `Assay` is automatically created and stored within the `Calibration Object`. `Assays` are organized data structures that contain the following slots:

- **data**: list of datasets (uncalibrated and/or calibrated)

- **analysis**: list of analyses performed on datasets (results from XcalRep functions with *Analysis* suffix are stored here)

- **calibration**: fitted curves used for cross calibration (see *fit.calibration()*)

- **plots**: list of plots (results from XcalRep function with *plot* suffix are stored here)

- **features**: features available in datasets

- **feature.types**: list of features and their unique types

- **N**: list of unique feature types and their counts

- **description**: assay name

In practice we may be interested in analyzing the same dataset using different analysis specifications. Rather than overwriting existing data, or handling multiple `Calibration Objects`, we can simply specify a new `Assay` within the same `Calibration Object`. Thus a `Calibration Object` can accomodate multiple assays, each representing a different version of the data and/or analysis. 

When multiple assays exist, XcalRep analyses are performed on the current assay. We can check the current assay using the `getAssay`. Since we just created the `Calibration Object`, our data is stored in an "input" `assay`. 

```{r get assay}
# get current assay
getAssay(co)
```

Each dataset within an `Assay` is designated either "uncalibrated" or "calibrated". The "calibrated" designation is set only after performing data calibration (*discussed in Section 5*). For now, input data is by default designated "uncalibrated". We can check the calibration status of datasets in an assay using `getDatasets`.

```{r get datasets}
# get existing datasets
getDatasets(co)
```

### 2.2 Checking available features

Take a look at which features have been imported into the `Calibration Object`. Note that if the `whichAssay` argument is not specified for `getFeatures`, the default `Assay` will be called. 

```{r get features}
# get list of available features
available.features <- getFeatures(object = co, which.assay = "input")

# show feature names
names(available.features)
```

We can also check how many unique feature types we have for each feature

```{r n unique features}
# get table of unique feature type counts
N.features <- getUniqueFeatureCount(object = co)

showTable(N.features) 
```

For details of which feature types are available for timepoint, section, phantom and parameter, we can check what data is stored in `available.features`

For example, the following lines of code inform us that we have data for 3 timepoints (0, 6 and 12 months), acquired from an EFP (european forearm phantom) scanned at 4 sections (labeled 1-4). 
. 
```{r available timepoints}
available.features[["timePoint"]]
```

```{r available phantom}
available.features[["phantom"]]
```

```{r available sections}
available.features[["section"]]
```

The following 16 parameters were measured in EFP
```{r available parameters}
available.features[["parameter"]]
```


## 3. Data Preprocessing

### 3.1 Get list of features to analyze
The first step of any calibration or precision analysis begins with preprocessing the data.

First we specify which feature types we want to include in our analysis using `analyzeWhich`. If inclusion or omission criteria for a given feature are not specified, all feature types are included. 

Since our data contains a single phantom (*See Section 2*), we will only analyze *sections 2-4*, and include measurements for parameters *"Tb. vBMD"*, *"Tt. vBMD"*, *"Tb. BVTV"*, *"ct. vBMD (XCTII)"*, and *"Ct. Th. (XCTII)"*. Sometimes it may be faster to omit undesired feature types instead of listing all types to include. This can be accomplished using an omission argument, as demonstrated for *"Tb. BVTV"* in this example. 

```{r these features}

# get list of feature subsets to analyze
analyze.these <- analyzeWhich(co, 
                              include.sections = seq(2,4), 
                              include.parameters = c("Tb. vBMD", "Tt. vBMD", "Tb. BVTV", "ct. vBMD (XCTII)", "Ct. Th. (XCTII)"),
                              omit.parameters = c("Tb. BVTV"))

# show list structure
str(analyze.these)
```

Alternatively, if we want to include all available feature types in our analysis, we can obtain a complete list of features to analyze:

```{r all features}
# get list of all features to analyze
analyze.all <- analyzeWhich(co)

# show list structure
str(analyze.all)
```

### 3.2 Prepare data for analysis

Once we have our list of features `analyze.these` to analyze (*Section 3.1*), we can preprocess our dataset. This will create a new `Assay` within our `Calibration Object`, taking the current `Assay` (i.e., "input") and filtering it to only include the subset of features that we are interested in for downstream analysis. The default `Assay` will automatically be set to the new preprocessed `Assay`, which we have named *"preprocessedData"*. 

```{r preprocess data, warning = F}
# preprocess data using specified features
co <- preprocessData(object = co, 
                      analyze.which = analyze.these, 
                      new.assay.name = "preprocessed.data",
                      which.assay = "input")
```

Now that we've created a new `Assay`, we have two assays stored in our `Calibration Object`, and we can check these using `getAssay`.

```{r get all assays}
getAssay(co, which.assays = "all")
```

All subseqent analyses will be performed on the current `Assay`, which in this case is "preprocessed.data". 

```{r get default assay}
getAssay(co, which.assays = "default")
```

## 4. Single-Variant Precision Analyses 

**Single-variant precision (SVP) analyses** aim to calculate the inherent variability of an instrument, without consideration for time- or scanner-dependent sources of error. That is, precision estimates are calculated by first computing replicate-level statistics (e.g., triplicate phantom scans at a single time point by a single scanner), and then pooled across all scanners and time points to yield a root-mean-square precision estimate. This is commonly known as a **short-term single-scanner/site precision** estimate and conveys meaning about how precision a given technology is at a given point in time, without consideration for external factors, such as drift over time, or between-scanner discrepancies. 

### 4.1 SVP Analysis

Lets calculate short-term single-site precision errors for our uncalibrated data stored in the preprocessed `Assay` using `svpAnalysis`.

```{r svp analysis, warning = F, message = F}

# calculate short-term precision errors for uncalibrated data
co <- svpAnalysis(co, 
                   which.data = "uncalibrated",
                   verbose = F)

```

*Tip:* Since the `svpAnalysis` contains an "*Analysis*" suffix, results are stored in the analysis slot of the current `Assay`. This pattern of assignment within the `Calibration Object` is consistent throughout the XcalRep Package. 

`svpAnalysis` generates two sets of tables which can be retrieved from the `Calibration Object` using `getResults`. The `replicate.statistics` table reports descriptive statistics for replicate scans and the `rms.statistics` table reports root-mean-square statistics (i.e., short-term single-site precision error estimates)

Let's take a look at the root-mean-square statistics. 

```{r get svp results}

# retrieve stored results (as datatables)
svp.results.uncalibrated <-getResults(object = co, 
                                which.results = "svp",
                                format = 'dt') # 'dt' (datatable) or 'df' (dataframe)

# see what tables were generated 
names(svp.results.uncalibrated)

# show rms statistics
showTable(svp.results.uncalibrated[["rms.statistics"]])

```


### 4.2 Outliers
Outliers are automatically flagged during SVP analysis and two sets of rms statistics tables are generated; `rms.statistics` contains statistics computed using all data, and `rms.statistics.no.outliers` contains results where outliers were omitted. Additionally, the `replicate.statistics` table contains an `outlier.flag` feature which flags suspected outliers. While it is generally poor practice to omit outiers from precision analyses, in the context of a multi-centre trial it may be informative to identify outlying sites. If outliers appear randomly distributed throughout the dataset and there is not discernable bias (e.g., all outlying data comes from a single site), it is recommended to retain all data for downstream analysis. If outlier prevalence is biased towards a certain site, users should investigate the source of error further. 

The easiest way to appraise outliers is to examine the `replicate.statistics` interactive datatable and sort entries using `outlier.flag`. 

```{r show outlier table}
# show replicate statistics
showTable(svp.results.uncalibrated[["replicate.statistics"]])
```
*Tip:* The `showTable` function provides users with an easy way to generate tables as interactive data tables (as seen above) or as data frames. Interactive data tables allow users to export results directy into their clipboard or to csv or excel, and enable easy data filtering, sorting and exploration in the R Studio Enviroment. 

### 4.3 Visualize Results

To visualize `svpAnalysis` results, use the `svpPlot` function. The default output plots CV-based precision errors, however STD-based precision errors can also be plotted using the `var2plot` argument. In this example we omit outliers and stratify the data by HR-pQCT scanner to determine whether 2nd generation XtremeCT scanners (XCT2) are more precise than 1st generation scanners (XCT1). 

Kruskal-Wallis rank sum test is also performed (using `group.by` feature for data stratification) and p-values are shown for each comparison group. Significant differences in precision errors are observed between XCT1 and XCT2 scanners. 


```{r svp plot scanner, fig.width= 7, fig.height = 5, warning = F, message=F}

svpPlot(co, 
         which.data = "uncalibrated",
         group.by = "scanner",
         outliers = F,
         color.begin = 0.1, color.end = 0.5)


```
We can also ask which parameters are most reproducible; The current dataset suggests that Ct. vBMD and Tt. vBMD measures are. 

```{r svp plot uncal, fig.width= 7, fig.height = 5, warning = F, message=F}

svpPlot(co, 
         which.data = "uncalibrated",
          outliers = F)

```

## 5 Cross-Calibration 

Imaging phantoms are used to cross-calibrate instruments/scanners thereby enabling direct comparion of measurements obtained by different instruments. In general, a reference instrument is selected, and pair-wise linear regression of imaging phantom measurements is used to calibrate all other instruments to the reference. For HR-pQCT, imaging phantoms (e.g., QC1, EFP) are designed to mimic varying degrees of bone mineral densities, along with certain other properties, thereby allowing calibration using multiple points of reference. 

### 5.1 Identify reference

Before we fit pair-wise calibration curves, we must first specify our point of reference with respect to the instrument (denoted `reference.site`) and time (if scans were obtained at multiple timepoints). 

We propose designating the reference site as the site that reports the most precise and accurate measurements. For a detailed description of how most precise and accurate site is derived, refer to our companion paper (**citation placeholder**). For the reference time, we recommend using baseline (t=0) measurements. In general, it is good practice to provide justification for the choice of reference. 

Using our running example, phantom measurements obtained by the "Oregon" scanner were the most precise and accurate. 

```{r reference site, warning = F, message = F}
reference.site <- identifyReference(co)
reference.site
```

Using the `consistencyPlot` function, we can explore how each site ranked across all parameters. Ranking is based on a minimized MSE-criteria, computed as the mean-squared error (MSE) between site-specific measures and the median measurement values pooled across all `timePoint` and `site` features. The `reference.site` is then selected as the site which has the highest average ranking across all parameters. The `tile` plot enables us to examine how each site ranked in a parameter-specific manner. 

```{r consistency tile, fig.width = 6, fig.height = 6, warning = F, message=F}
consistencyPlot(co, which.plot = "tile")
```

Alternatively, `box` plot shows the overall ranking once parameter-specific ranks are pooled. 
```{r consistency box, fig.width = 6, fig.height = 4, warning = F, message=F}
# Display table of MSE estimates (used to identify reference site)
# showTable(consistencyAnalysis(co), cast.as.dt = T)

consistencyPlot(co, which.plot = "box")
```

### 5.2 Fit calibration curves

Compute calibration equations for the designated reference site using `fitCalibration`. 

```{r fit calibration, warning = F, message=F}
# fit calibration curves
co <- fitCalibration(co, 
                      reference.site = reference.site,
                      sig.intercept.only = F,
                      which.assay = NULL)
```
*Tip:* Cross-calibration requires atleast 3 calibration points in each phantom. If less than 3 imaging phantom sections are detected in the data, an error will occur notifying the user. 

### 5.3 Calibration equations

Once calibration equation have been computed, we can retrieve the results from the `Calibration Object` using `getResults`. Results can be directly exported to a spreadsheet from the interactive data table. 
```{r get calibration results, warning = F, message=F}

# retrieve calibration results (as datatables)
calibration.results <-getResults(object = co, 
                                which.results = "calibration",
                                format = 'dt') 

# see what tables were generated 
names(calibration.results)

# show rms statistics
showTable(calibration.results[["calibration.equations"]])
```

### 5.4 Visualization

To visualize the calibration curves, use the `calibrationPlot` function. 
```{r calibration curves, fig.width = 7, fig.height = 5}
calibrationPlot(co, 
                 which.parameter = "Tb. vBMD", 
                 return.plt.handle = F, 
                 color.option = "A", color.begin = 0, color.end = 0.5)
```

### 5.5 Data calibration

To calibrate phantom data using the computed calibration equations, use the `calibrateData` function. This will generate a new dataset called `calibrated.data` within the current `Assay`. 

```{r calibrate data}
# calibrate data
co <- calibrateData(co, verbose = F)
```

The current `Assay` now contains two datasets; `uncalibrated` and `calibrated`.
```{r get data calibration}
# show available datasets
getDatasets(co)
```

### 5.6 Diagnostics

A **diagnostic plot** comparing pre- and post-calibrated values is useful in determining whether all sites were successfully calibrated. XcalRep enables users to generate three kinds of diagnostic plots using the `diagnosticPlot` function. 

**1) Diagnostic Line Plot:** 
Pre- and post-calibration values are plotted on the x and y axes, respectively, and 2 sets of reference curves are overlaid; Horizontal dashed references denote section-specific reference values which were used for calibration. The diagonal dashed reference is the line of equality (x = y) and the reference site lies along this curve.  

```{r diagnostic line, fig.width= 7, fig.height = 4}
diagnosticPlot(co, which.parameter = "Tb. vBMD", which.plot = "line")
```

**2) Diagnostic Bar Plot:** 
Pre- and post-calibration values are plotted on a horizontal bar plot, and the median value is shown as a dashed reference. 

```{r diagnostic bar, fig.width= 7, fig.height = 6, message = F}
diagnosticPlot(co, which.parameter = "Tb. vBMD", which.plot = "bar", fix.axis = T)
```

**3) Diagnostic Residual Distributions:** 
The distribution of pre- and post-calibration residuals is visualized using a histrogram and overlaid density curve. Residuals are here defined as the difference between the replicate mean and pooled median. 

```{r residuals, fig.width= 7, fig.height = 5, message = F}
diagnosticPlot(co, which.parameter = "Tb. vBMD", which.plot = "residual", fix.axis = T)
```


## 6. Multi-Variant Precision Analyses 

**Multi-variant precision (MVP)** analysis aims to calculate the inherent variability of an instrument and additional error arising from time- or scanner-dependent drifts in scanner calibration. MVP analysis is commonly referred to as **multisite-longitudinal precision** or reproducibility analysis and is necessary to perform when conducting a multisite or longitudinal clincial trial involving patient scans over multiple scanners and timepoints. 

### 6.1 MVP Analysis

Run `mvpAnalysis` to perform MVP analysis and store results in `Calibration Object`. By specifying which.data = "all", MVP analysis will be performed on all data in the current `Assay`, and stratified by calibration (uncalibrated vs. calibrated) status.

```{r mvp analysis}

co <- mvpAnalysis(co, 
                   which.data = "all",  
                   verbose = F)


# retrieve stored results (as datatables)
mvp.results <-getResults(object = co, 
                                which.results = "mvp",
                                format = 'dt') 

# show rms statistics for calibrated data
showTable(mvp.results[["rms.statistics"]])

```
Alternatively, we can focus on just multisite longitudinal precision error

```{r}
# retrieve stored results (as datatables)
mvp.results <-getResults(object = co, 
                                which.results = "mvp",
                                format = 'df') 

# get rms statistics
mvp.rms <- mvp.results[["rms.statistics"]]

# subset rms statistics
mvp.rms.subset <- mvp.rms[grepl(c("long-t12.multi"), mvp.rms$precision.type),  ]

# show table
showTable(mvp.rms.subset, as.dt = T)

```


### 4.3 Visualize Results

To visualize `mvpAnalysis` results, use the `mvpPlot` function. Function arguments are similar to those in the `svpPlot` function, with the additional option to subselect different precision error types, including:

- **short**: Short-term precision errors only

- **long**: Longitudinal precision errors (i.e., long-term) only

- **single**: Single-site precision errors only

- **multi**: Multi-site precision errors only

- **all**: all of the above (default)


```{r mvp plot}

mvpPlot(co, 
         outliers = T, 
         var2plot = "cv",
         which.data = "all", 
         which.parameter = "Tb. vBMD", 
         which.precision = "all",
         color.begin = 0.1, 
         color.end = 0.5,
         jitter.width = 0.1,
         show.rms.statistic = T)
```


## 7. Miscellaneous Functions

There are several additional functions that are worthwhile exploring in the `XcalRep` package.

**1) Mean-Variance Relationship:** `meanVarPlot`

The `mean.var.plot` function enables users to explore the relationship between measurement means and variance, and may inform which precision error is more appropriate to report. The precision error measure that remains relatively constant across all values is preferred, as it does not misrepresent reproducibility for extreme values. 

This relationship can be examined as either a **scatter plot**

```{r meanVarPlot scatter}
meanVarPlot(co, 
              which.data = "calibrated", 
              which.plot = "scatter", 
              which.parameter = "Tb. vBMD")
```

or **boxplot**

```{r meanVarPlot box, fig.width= 10, fig.height= 3.5}
meanVarPlot(co, 
              which.data = "calibrated", 
              which.plot = "box", 
              which.parameter = "Tb. vBMD")
```



**2) Utility Functions:** 

`cloneAssay`
If users i) create an analysis "checkpoint" to which they can return at a later time or ii) diverge in analysis pipelines for the current dataset, `cloneAssay` will duplicate the specified (or current) `Assay` within the current `Calibration Object`.  

```{r clone assay}
# duplicate current assay
co <- cloneAssay(co)
getAssay(co, which.assays = "all")
```



`deleteAssay`
A specified assay can be deleted using the `deleteAssay` function. 

```{r delete assay}
# duplicate current assay
co <- deleteAssay(co, which.assay = "preprocessed.data-copy")
```

